\documentclass{scrartcl}
\usepackage[margin=1in]{geometry}
\usepackage{url}

\title{Jurassic Parkour}
\subtitle{Reinforcement Learning in an HTML5 Browser Game}
\author{Sebastian Laudenschlager, Ian Martiny}

\begin{document}
\maketitle

\section{Introduction}

The goal of this project is to use reinforcement learning to train a computer agent to play the Chrome T-Rex game. To that end, we use data that is procedurally generated by running the game. The agent has a set of actions, and the environment has a set of states, and the agent will choose an action based on the state of the environment. To make decisions, act accordingly, and learn from the result, we utilized the REINFORCEjs library \cite{reinforcejs}, which implements some reinforcement learning algorithms such as Deep Q-Learning. The platform we use to test the learning algorithm is the Javascript T-Rex game running on a locally hosted python server using Flask. We first discuss some of the previous related work that this project was built on, followed by a more detailed look at the methods and algorithms we used. Subsequently, we present some data from several experimental runs and comment on the performance of the agent. Finally, we provide some commentary on the successes and failures of this project, as well as some ways to improve on this project.

\section{Related Work}

\section{Methods}

    \subsection{Baselines}
    Prior to testing the reinforcement learning algorithm, we established two baseline tests for the T-Rex game: (1) Random jumping and (2) Increasing Threshold.

    \par The random jumping baseline consists of the python server computing a random threshold between 0 and 1 and sending it to the Javascript client. The client then computes its own random threshold value between 0 and 1, and then compares the two threshold values. If the client's value is larger, the T-rex jumps, otherwise nothing happens. We do this for every run, meaning that once the T-Rex ``dies'', the game resets, and new values are computed. We also keep track of the score for each run, and then take the average over many runs.

    \par The increasing threshold baseline works as follows:

    \begin{itemize}
      \item If the reported score is between 1000 and 2000 (inclusive) then the new
        threshold is increased by a tenth of the distance from 1. These are
        considered low scores. A score less than 1000 is not possible. This allows
        the threshold to increase by a significant amount, without going above 1.
      \item If the reported score is between 2001 and 4000 (inclusive) then the new
        threshold is increased by a fiftieth of the distance from 1. These are still
        relatively bad scores, but realistically the best we can hope for with
        purely random jumping. For this model these scores are considered good
        enough to not change much.
      \item If the reported score is anything higher the threshold is increased by a
        hundredth of the distance from 1. Any score above 4000 is considered very
        good for this model and thus the threshold should not change very much.
    \end{itemize}

    We again keep track of the scores for many runs, and then take the average of the scores.

    \subsection{Reinforcment Learning}
    To actually train a computer agent to learn to play the T-Rex game, we used a reinforcement learning algorithm, specifically the Deep Q-Learning Algorithm from the REINFORCEjs library. The agent essentially learns in the following way:

    \begin{itemize}
        \item First, we initialize the agent with a set of parameters including the learning rate $\alpha$, the greedy policy $\epsilon$, the learning algorithm, experience size, etc.
        \item We then start a loop to keep playing the game. At the beginning of every run, we reset the environment states.
        \item Based on the state of the current environment, the agent chooses and executes an action.
        \item Based on the result of the action, a reward / penalty is applied.
        \item The agent then updates its policy accordingly, and if the T-Rex ``died'', the game restarts. If the agent successfully avoided an obstacle, the environment is updated to reflect the next obstacle, and the process repeats.
    \end{itemize}

    The environment states at any point in the game are described by four variables:
    \begin{itemize}
        \item The current speed of the game, i.e. how fast obstacles are moving toward the T-Rex
        \item The height (y-position) of the T-Rex
        \item The horizontal distance between the nearest obstacle and the T-Rex
        \item The height of the nearest obstacle
    \end{itemize}

    The action space for the agent is (1) Idle (do nothing), (2) Jump, and (3) Duck. Due to the continuous updating of the environment states, we have continuous state features and discrete actions, which led us to using REINFORCEjs's \textbf{DQNAgent}.

\section{Results}

\section{Conclusion}




\end{document}
